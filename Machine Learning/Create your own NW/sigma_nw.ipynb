{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Модель нейронной сети, которая позволяет проводить распознование рукописных цифр. \n",
    "\n",
    "Модель выполнена по книге *Тарик Рашид - Создаем нейронную сеть* (https://www.ozon.ru/context/detail/id/141796497/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процесс.\n",
    "\n",
    "Допустим у нас трехслойная нейронная сеть, со слоями соответственно $input \\rightarrow hidden \\rightarrow output$.\n",
    "\n",
    "В процессе обучения выделяют следующие шаги.\n",
    "\n",
    "1. Прямой проход (суммирование всех взвешенных input-ов). Получаем промежуточный output.\n",
    "$$\n",
    "    o_{hid} = W_{inp, hid} \\times I_{inp}, \\\\\n",
    "    o_{out} = W_{hid, out} \\times I_{hid}\n",
    "$$\n",
    "\n",
    "2. Применяем функцию активации к каждому из промежуточных output-ов.\n",
    "$$\n",
    "    o_{hid} = sigm(W_{inp, hid} \\times I_{inp}), \\\\\n",
    "    o_{out} = sigm(W_{hid, out} \\times I_{hid})\n",
    "$$\n",
    "\n",
    "3. Вычисление ошибки предсказания на output слое (используем квадратичную ошибку).\n",
    "$$\n",
    "    E_{out} = (expect_{out} - o_{out})^{2}\n",
    "$$\n",
    "\n",
    "4. Обратное распростронение ошибки (вычисление ошибки для hidden слоя)\n",
    "$$\n",
    "    E_{hid} = W_{hid, out}^{T} \\times E{out}\n",
    "$$\n",
    "\n",
    "P.S. Oшибка каждого узла делится пропорционально весам, входящим в данный. Мы просто не нормируем на сумму всех весов по столбцу, так как это не особо важно.\n",
    "\n",
    "5. Обновление весовых коэффициентов.\n",
    "$$\n",
    "    W_{hid, out} = W_{hid, out} - \\alpha \\frac{\\partial E_{out}}{\\partial W_{hid, out}}, \\\\\n",
    "    W_{inp, hid} = W_{inp, hid} - \\alpha \\frac{\\partial E_{hid}}{\\partial W_{inp, hid}}\n",
    "$$\n",
    "\n",
    "\n",
    "Распишем последний пункт конкретно для нашей ситуации (квадратичная ошибка, функция активации - сигмиоида).\n",
    "$$\n",
    "    \\frac{\\partial \\; E_{out}}{\\partial w_{hid, out}} = \n",
    "    \\frac{\\partial \\; E_{out}}{\\partial o_{out}} \\times \\frac{\\partial \\; o_{out}}{\\partial w_{hid, out}},\n",
    "$$\n",
    "\n",
    "Но ввиду (см. ранее $E_{out}$):\n",
    "$$\n",
    "    \\frac{\\partial \\; E_{out}}{\\partial o_{out}} = -2 \\times (expect_{out} - o_{out}),\n",
    "$$\n",
    "\n",
    "И кроме того ввиду (см. ранее $o_{out}$ и учитывая что $sigm(x)' = sigm(x) \\times (1 - sigm(x))$), \n",
    "$$\n",
    "    \\frac{\\partial \\; o_{out}}{\\partial w_{hid, out}} = \n",
    "    sigm(W_{hid, out} \\times I_{hid}) \\times (1 - sigm(W_{hid, out} \\times I_{hid})) * o_{hid}\n",
    "$$\n",
    "\n",
    "P.S. Последнее слагаемое получается по правилу дифференцирования сложной функции.\n",
    "\n",
    "Таким образом:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\; E_{out}}{\\partial w_{hid, out}} = -2 \\times (expect_{out} - o_{out}) \\times \n",
    "    sigm(W_{hid, out} \\times I_{hid}) \\times (1 - sigm(W_{hid, out} \\times I_{hid})) * o_{hid}\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# For SIGMA() function\n",
    "import scipy.special\n",
    "\n",
    "class neuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_noes, hidden_nodes, out_nodes, learning_rate):\n",
    "        self.inodes = input_noes\n",
    "        self.hnodes = hidden_nodes\n",
    "        self.onodes = out_nodes\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Задаются как выборка из нормального распределения с центром в нуле, и мат. ожиданием равным\n",
    "        # 1. / (корень квадратный из количества узлов)\n",
    "        # Weights between input and hidden layers\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        # Weights between hidden and output layers\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        # Activation function\n",
    "        self.act_func = lambda x: scipy.special.expit(x)\n",
    "    \n",
    "    # NW train function\n",
    "    def train(self, input_list, target_list):\n",
    "        inputs = np.array(input_list, ndmin = 2).T\n",
    "        targets = np.array(target_list, ndmin = 2).T\n",
    "        \n",
    "        # Caluclate output of hidden layer\n",
    "        hidden_input = np.dot(self.wih, inputs)\n",
    "        hidden_output = self.act_func(hidden_input)\n",
    "        \n",
    "        # Calculate output of output layers\n",
    "        out_input = np.dot(self.who, hidden_output)\n",
    "        out_output = self.act_func(out_input)\n",
    "        \n",
    "        # Calculate error of output layer\n",
    "        out_errors = targets - out_output\n",
    "        \n",
    "        # Reverse error correction on output->hidden layer\n",
    "        hidden_errors = np.dot(self.who.T, out_errors)\n",
    "        self.who += self.lr * np.dot( (out_errors * out_output * (1.0 - out_output)), np.transpose(hidden_output) )\n",
    "        \n",
    "        # Reverse error correction on hidden->input layer\n",
    "        self.wih += self.lr * np.dot( (hidden_errors * hidden_output * (1.0 - hidden_output)), np.transpose(inputs) )\n",
    "        \n",
    "    \n",
    "    # Calculate output results based on choosen 'input_list' data\n",
    "    def query(self, inputs_list):\n",
    "        inputs = np.array(inputs_list, ndmin = 2).T\n",
    "        \n",
    "        hidden_input = np.dot(self.wih, inputs)\n",
    "        hidden_output = self.act_func(hidden_input)\n",
    "        \n",
    "        out_input = np.dot(self.who, hidden_output)\n",
    "        out_output = self.act_func(out_input)\n",
    "        \n",
    "        return out_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_nodes = 784\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "# Number of repetition for NW on single dataset\n",
    "epoch_number = 5\n",
    "\n",
    "train_file_path = os.path.join(os.getcwd(), 'mnist_dataset', 'mnist_train.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nw = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "    \n",
    "    # Prepare trainig data\n",
    "    trainig_data_file = open(train_file_path, 'r')\n",
    "    train_data_list = trainig_data_file.readlines()\n",
    "    trainig_data_file.close()\n",
    "    \n",
    "    for i in range(epoch_number):\n",
    "        for record in train_data_list:\n",
    "            all_values = record.split(',')\n",
    "            # Create data for input nodes\n",
    "            # Convert from [0,255] range to [0.01, 1.0]\n",
    "            input_data = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "            # Create output targets\n",
    "            # If output is 5 -> [0.01, 0.01, 0.01, 0.01, 0.99, 0.01, 0.01, 0.01, 0.01,]\n",
    "            output_data = np.zeros(output_nodes) + 0.01\n",
    "            output_data[int(all_values[0])] = 0.99\n",
    "\n",
    "            nw.train(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NW efficiency: 0.962\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_file_path = os.path.join(os.getcwd(), 'mnist_dataset', 'mnist_test.csv')\n",
    "\n",
    "test_data_file = open(test_file_path, 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "\n",
    "scores = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    for record in test_data_list:\n",
    "        test_values = record.split(',')\n",
    "        test_input = (np.asfarray(test_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        expected_value = int(test_values[0])\n",
    "        #print('Expected value: {0}'.format(expected_value))\n",
    "        #plt.imshow(test_input.reshape(28,28), cmap='Greys', interpolation='None')\n",
    "        #plt.show()\n",
    "\n",
    "        train_values = nw.query(test_input)\n",
    "        \n",
    "        if expected_value == np.argmax(train_values):\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    \n",
    "    scores = np.asarray(scores)\n",
    "    print('NW efficiency: {0}'.format(float(np.sum(scores)) / float(scores.size)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
