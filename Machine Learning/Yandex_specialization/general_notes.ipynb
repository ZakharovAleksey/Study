{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "**Overfitting** - state, where your model is well trained on train dataset, but could not predict on test dataset, because it reminds only train dataset, and could not determine main properties in data.\n",
    "\n",
    "1. In linear models overfitting is notisible when weights of features are huge - to deal with it we use **regularization**.\n",
    "\n",
    "2. Regularization - fine model for extra complexity:\n",
    "    - $L2$ - good for gradient descent. Most offen choise.\n",
    "    - $L1$ - could help to select important features (set weights of less importaant features to zero).\n",
    "\n",
    "3. Algorithm quality estimation:\n",
    "    - Big dataset: delayed sampling (1 item in crossvalidation)\n",
    "    - Little dataset:Crossvalidation (train/test data) in 70/30 ; 80/20; .632/.368. K (number of butches) 3,5,7\n",
    "    - Always shuffle data\n",
    "\n",
    "4. When we chose between several algorithms using delayed sampling: which is best for these type of data we should split on 3 parts: train, validation, test!!!\n",
    "    - This happens, because in case of 2-split, we overfit best algoeithm on train data -> so we substitute algorithm for these spesific data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality metrics\n",
    "\n",
    "1. Loss function\n",
    "\n",
    "\n",
    "Regression quality:\n",
    "1. MSE = ! Not good for data with outlines\n",
    "2. MAE = ! Good for data with outlines\n",
    "3. Determination R = 1 - (какая доля дисперсии (разнообразия) объяснено нашей моелью)\n",
    "    - R = 0 - constant model\n",
    "    - R = 1 - ideal model\n",
    "    - R < 0 - model is worser than constant\n",
    "4. Quantile error - qualtiy with different weights for correct/incorrect prediction (**usimmetric quality function**) \n",
    "\n",
    "\n",
    "Classification quality:\n",
    "1. accuracy (доля правильных ответов) [bad in unbalanced datasets, do not takes into account differnet weights of errors]\n",
    "2. точность - precision = TP/(TP+FP)  делим на сумму в первой строке. Точность показывает сколько из выделенных алгоритмом объектов на самом деле относится к классу 11.\n",
    "3. полнота - recall = TP / (TP + FN) делим на сумму в первом столбце Полнота показывает как много положительных объектов находит алгоритм.\n",
    "Error matrix.\n",
    "\n",
    "Often: we have constraints on one among two, and try to maximize - other.\n",
    "Examples: medical classification, credit scoring.\n",
    "\n",
    "Unite recall & precsition.\n",
    "F = 2 * precision * recall / (precision + recal)\n",
    "\n",
    "https://www.coursera.org/learn/supervised-learning/lecture/f2X6o/kachiestvo-otsienok-prinadliezhnosti-klassu\n",
    "\n",
    "PR-curve (precision, recall) - better with disbalanced classes\n",
    "- AUC-PRC square unred PR curve\n",
    "\n",
    "ROC curve  (does not depend on classes proportions!!!!) 0.5 values - bad? like random gessing.\n",
    "Ox = False Positive Rate\n",
    "Oy = True Positive Rate\n",
    "AUC-ROC curve\n",
    "Surface under curve/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
